# FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability

Fairness Benchmarking toolkit!

This includes data loader, custom dataset support, different fairness models, and wide range of evaluations. 

![FairX](fig/fairx-extended.png)


## Installation

```terminal
conda create -n fairx python=3.8
conda activate fairx

git clone git@github.com/fahim-sikder/fairx.git

cd fairx

pip install .
```


## Fair Models
### Pre-processing

- [x] Correlation Remover

### In-processing

- [x] TabFairGAN
- [x] Decaf
- [x] FairDisco
- [ ] FLDGMs
### Post-processing

- [x] Threshold Optimizers

## Evaluation Metrics

### Fairness Evaluation

- [x] Demographic Parity Ratio (DPR)
- [x] Equilized Odds Ratio (EOR)
- [x] Fairness Through Unawareness (FTU)
- [ ] Intersectional Bias (IB)

### Data Utility

- [x] Accuracy
- [x] AUROC
- [x] F1-score
- [x] Precision
- [x] Recall

### Synthetic Data Evaluation

- [x] $\alpha-$ precision
- [x] $\beta-$ recall
- [x] Authenticity

## Available Dataset

| **Dataset Name**    | **Protected Attribute**      | **Target Attribute** | **Dataset Type** |
|---------------------|------------------------------|----------------------|------------------|
| Adult-Income        | sex<br>race                  | class                | Tabular          |
| Compass             | sex<br>race_African-American | two_year_recid       | Tabular          |
| Student-performance | sex                          | Pstatus              | Tabular          |
| Predict-diagnosis   | Sex<br>Race                  | Diagnosis            | Tabular          |
| ColorMNIST          | color                        | class                | Image            |
| CelebA              | Eyeglasses                   | Gender               | Image            |

## Usage

### Dataset loading

```python
from fairx.dataset import BaseDataClass

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = True

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target = attach_target)

print(data_class.data.head())
```

### Custom Dataset Loading

```python
from fairx.dataset import CustomDataClass

dataset_path = 'Random-dataset.csv'
sensitive_attr = 'some-sensitive-attribute'
target_attr = 'some-target-feature'
attach_target = True

custom_data_class = CustomDataClass(dataset_path, sensitive_attr, target_attr, attach_target)

print(custom_data_class.data.head())
```

### Model Loading

```python
from fairx.dataset import BaseDataClass

from fairx.models.inprocessing import TabFairGAN

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = True

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target = attach_target)

under_prev = 'Female'
y_desire = '>50K'

tabfairgan = TabFairGAN(under_prev, y_desire)

tabfairgan.fit(data_class, batch_size = 256, epochs = 5)
```

### Evaluation Utility

```python
from fairx.dataset import BaseDataClass
from fairx.metrics import FairnessUtils, DataUtilsMetrics

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = False

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target = attach_target)

_, _, tf_data = data_module.preprocess_data()

splitted_data = data_module.split_data(tf_data)

## Data Utility
data_utils = DataUtilsMetrics(splitted_data)
utils_res = data_utils.evaluate_utility()
print(utils_res)

## Fairness
fairness_eval = FairnessUtils(splitted_data)
fairness_res = fairness_eval.evaluate_fairness()
print(fairness_res)
```

## Results

PCA and t-SNE plots of fair synthetic data, generated by TabFairGAN.

![PCA and t-SNE Plots](fig/tsne.png)

Intersectional Bias on `Adult-Income` dataset.

![Intersectional Bias](fig/ib.png)

### Model's performance on Data utiliy vs Fairness

Here, we have compared all the model in our benchmarking tools on Data utility vs Fairness metrics. For the data utlity, we calculate the Accuracy and for the fairness, we measure Demographic Parity Ration (DPR) and Equilized Odds Ratio (EOR), and plot them in 3d.

![Fairness vs Data utlity](fig/fairnessvsdata.png)

### Image Results

#### Color MNIST

FairDisco on Color MNIST:

Here color is the `Senstitive attribute`!

Generated sample of all sensitive Attributes.

![Mix ColorMNIST data](fig/mix-cmnist.png)

Generated sample when sensitive Attributes is `red`!

![Red ColorMNIST data](fig/red-cmnist.png)

<!-- ![Green ColorMNIST data](fig/green-cmnist.png)

![Blue ColorMNIST data](fig/blue-cmnist.png) -->

#### CelebA



More results coming soon!

## References

1. https://github.com/fairlearn/fairlearn
2. https://github.com/Trusted-AI/AIF360
3. https://github.com/shap/shap