# FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability

<h1 align="center">
    Fairness Benchmarking toolkit! 
</h1>

<h3 align="center">
    Proceedings of the 2nd Workshop on Fairness and Bias in AI, co-located with 27th European Conference on Artificial Intelligence (ECAI 2024)
</h3>



<div align="center">

[![python](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue.svg)](https://www.python.org/)
![Build](https://img.shields.io/github/actions/workflow/status/fahim-sikder/FairX/python-package.yml)
![License](https://img.shields.io/github/license/fahim-sikder/FairX)

</div>


This includes data loader, custom dataset support, different fairness models, and wide range of evaluations. 




![FairX](fig/fairx-extended.png)

## Latest News (October 2025)

**Our paper titled `Promoting Intersectional Fairness through Knowledge Distilaltion` has been accepted to the ECAI 2025. We will release and merge the codebase with FairX.**

## Installation

```terminal
conda create -n fairx python=3.8
conda activate fairx

git clone git@github.com/fahim-sikder/fairx.git

cd fairx

pip install .
```


## Fair Models
### Pre-processing

- [x] Correlation Remover

### In-processing

- [x] TabFairGAN
- [x] Decaf
- [x] FairDisco
- [ ] FLDGMs
- [ ] Promoting Intersectional Fairness through KD
### Post-processing

- [x] Threshold Optimizers

## Evaluation Metrics

### Fairness Evaluation

- [x] Demographic Parity Ratio (DPR)
- [x] Equilized Odds Ratio (EOR)
- [x] Fairness Through Unawareness (FTU)
- [ ] Intersectional Bias (IB)

### Data Utility

- [x] Accuracy
- [x] AUROC
- [x] F1-score
- [x] Precision
- [x] Recall

### Synthetic Data Evaluation

- [x] $\alpha-$ precision
- [x] $\beta-$ recall
- [x] Authenticity

## Available Dataset

| **Dataset Name**    | **Protected Attribute**      | **Target Attribute** | **Dataset Type** |
|---------------------|------------------------------|----------------------|------------------|
| Adult-Income        | sex<br>race                  | class                | Tabular          |
| Compass             | sex<br>race_African-American | two_year_recid       | Tabular          |
| Student-performance | sex                          | Pstatus              | Tabular          |
| Predict-diagnosis   | Sex<br>Race                  | Diagnosis            | Tabular          |
| ColorMNIST          | color                        | class                | Image            |
| CelebA              | Eyeglasses                   | Gender               | Image            |

## Usage

**Beside the following example, please check the `tutorials` folder in the repo!**

### Dataset loading

```python
from fairx.dataset import BaseDataClass

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = True

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target = attach_target)

print(data_class.data.head())
```

### Custom Dataset Loading

```python
from fairx.dataset import CustomDataClass

dataset_path = 'Random-dataset.csv'
sensitive_attr = 'some-sensitive-attribute'
target_attr = 'some-target-feature'
attach_target = True

custom_data_class = CustomDataClass(dataset_path, sensitive_attr, target_attr, attach_target)

print(custom_data_class.data.head())
```

### Model Loading

```python
from fairx.dataset import BaseDataClass

from fairx.models.inprocessing import TabFairGAN

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = True

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target = attach_target)

under_prev = 'Female'
y_desire = '>50K'

tabfairgan = TabFairGAN(under_prev, y_desire)

tabfairgan.fit(data_class, batch_size = 256, epochs = 5)
```

### Evaluation Utility

```python
from fairx.dataset import BaseDataClass
from fairx.metrics import FairnessUtils, DataUtilsMetrics

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = False

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target = attach_target)

_, _, tf_data = data_module.preprocess_data()

splitted_data = data_module.split_data(tf_data)

## Data Utility
data_utils = DataUtilsMetrics(splitted_data)
utils_res = data_utils.evaluate_utility()
print(utils_res)

## Fairness
fairness_eval = FairnessUtils(splitted_data)
fairness_res = fairness_eval.evaluate_fairness()
print(fairness_res)
```

## Results

PCA and t-SNE plots of fair synthetic data, generated by TabFairGAN.

![PCA and t-SNE Plots](fig/tsne.png)

Intersectional Bias on `Adult-Income` dataset.

![Intersectional Bias](fig/ib.png)

### Model's performance on Data utiliy vs Fairness

Here, we have compared all the model in our benchmarking tools on Data utility vs Fairness metrics. For the data utlity, we calculate the Accuracy and for the fairness, we measure Demographic Parity Ration (DPR) and Equilized Odds Ratio (EOR), and plot them in 3d.

![Fairness vs Data utlity](fig/fairnessvsdata.png)

### Image Results

#### Color MNIST

FairDisco on Color MNIST:

Here color is the `Senstitive attribute`!

![Mix ColorMNIST data](fig/cmnist.png)

<!-- ![Green ColorMNIST data](fig/green-cmnist.png)

![Blue ColorMNIST data](fig/blue-cmnist.png) -->

#### CelebA

FairDisco on CelebA dataset:

![Celeba](fig/celeba.png)

## Tabular Results

**Dataset: `Predict-diagnosis`, Sensitive_attr: `Sex`**



| Methods             |   Precision |   Recall |   Accuracy |   F1 Score |    Auroc |   Demographic Parity Ratio |   Equalized Odd Ratio |   Alpha-precision |   Beta-recall |   Authenticity |
|:--------------------|------------:|---------:|-----------:|-----------:|---------:|---------------------------:|----------------------:|------------------:|--------------:|---------------:|
| Correlation Remover |    0.93783  | 0.947836 |   0.941212 |   0.942807 | 0.94106  |                   0.700837 |              0.59161  |        n/a        |    n/a        |     n/a        |
| Threshold Optimizer |    0.940211 | 0.9508   |   0.943939 |   0.945476 | 0.943782 |                   0.938307 |              0.183804 |        n/a        |    n/a        |     n/a        |
| FairDisco           |    0.561947 | 0.538592 |   0.527727 |   0.550022 | 0.526887 |                   0.956767 |              0.824377 |        n/a        |    n/a        |     n/a        |
| TabFairGAN          |    0.965393 | 0.964621 |   0.964172 |   0.965007 | 0.964161 |                   0.665885 |              0.713686 |          0.85877  |      0.354679 |       0.586545 |
| Decaf               |    0.981911 | 0.979143 |   0.977    |   0.980525 | 0.976521 |                   0.552198 |              0.281135 |          0.704436 |      0.32964  |       0.5996   |




More results coming soon!

## Citation

If you use our benchmark, please cite our work.

```bibtex
@inproceedings{sikder2024fairx,
	title={FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and eXplainability},
	author={Sikder, Md Fahim and Ramachandranpillai, Resmi and de Leng, Daniel and Heintz, Fredrik},
	booktitle={Proceedings of the 2nd Workshop on Fairness and Bias in AI, co-located with 27th European Conference on Artificial Intelligence (ECAI 2024)},
	year={2024},
	organization={CEUR-WS.org/Vol-3808}
}
```
```bibtex
@incollection{ramachandranpillai2023fair,
  title={Fair Latent Deep Generative Models (FLDGMs) for Syntax-Agnostic and Fair Synthetic Data Generation},
  author={Ramachandranpillai, Resmi and Sikder, Md Fahim and Heintz, Fredrik},
  booktitle={ECAI 2023},
  pages={1938--1945},
  year={2023},
  publisher={IOS Press}
}
```
```bibtex
@incollection{fahimpromoting2025,
  title={Promoting Intersectional Fairness Through Knowledge Distillation},
  author={Sikder, Md Fahim and Ramachandranpillai, Resmi and De Leng, Daniel and Heintz, Fredrik},
  booktitle={ECAI 2025},
  pages={3427--3434},
  year={2025},
  publisher={IOS Press}
}
```


## References

1. https://github.com/fairlearn/fairlearn
2. https://github.com/Trusted-AI/AIF360
3. https://github.com/shap/shap
