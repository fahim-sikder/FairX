# FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability

Fairness Benchmarking toolkit!

This includes data loader, custom dataset support, different fairness models, and wide range of evaluations. 

![FairX](fig/fairx-extended.png)


## Installation

```terminal
conda create -n fairx python=3.8
conda activate fairx

git clone git@github.com/fahim-sikder/fairx.git

cd fairx

pip install .
```


## Fair Models
### Pre-processing

- [x] Correlation Remover

### In-processing

- [x] TabFairGAN
- [x] Decaf
- [x] FairDisco
- [ ] FLDGMs
### Post-processing

- [x] Threshold Optimizers

## Evaluation Metrics

### Fairness Evaluation

- [x] Demographic Parity Ratio (DPR)
- [x] Equilized Odds Ratio (EOR)
- [x] Fairness Through Unawareness (FTU)
- [ ] Intersectional Bias (IB)

### Data Utility

- [x] Accuracy
- [x] AUROC
- [x] F1-score
- [x] Precision
- [x] Recall

### Synthetic Data Evaluation

- [x] $\alpha-$ precision
- [x] $\beta-$ recall
- [x] Authenticity

## Available Dataset

| **Dataset Name**    | **Protected Attribute**      | **Target Attribute** | **Dataset Type** |
|---------------------|------------------------------|----------------------|------------------|
| Adult-Income        | sex<br>race                  | class                | Tabular          |
| Compass             | sex<br>race_African-American | two_year_recid       | Tabular          |
| Student-performance | sex                          | Pstatus              | Tabular          |
| Predict-diagnosis   | Sex<br>Race                  | Diagnosis            | Tabular          |
| ColorMNIST          | color                        | class                | Image            |
| CelebA              | Eyeglasses                   | Gender               | Image            |

## Usage

### Dataset loading

```python
from fairx.dataset import BaseDataClass

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = True

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target)

print(data_class.data.head())
```

### Custom Dataset Loading

```python
from fairx.dataset import CustomDataClass

dataset_path = 'Random-dataset.csv'
sensitive_attr = 'some-sensitive-attribute'
target_attr = 'some-target-feature'
attach_target = True

custom_data_class = CustomDataClass(dataset_path, sensitive_attr, target_attr, attach_target)

print(custom_data_class.data.head())
```

### Model Loading

```python
from fairx.dataset import BaseDataClass

from fairx.models.inprocessing import TabFairGAN

dataset_name = 'Adult-Income'
sensitive_attr = 'sex'
attach_target = True

data_class = BaseDataClass(dataset_name, sensitive_attr, attach_target)

under_prev = 'Female'
y_desire = '>50K'

tabfairgan = TabFairGAN(under_prev, y_desire)

tabfairgan.fit(data_class, batch_size = 256, epochs = 5)
```

### Evaluation Utility

Coming Soon!

## Results

PCA and t-SNE plots of fair synthetic data, generated by TabFairGAN.

![PCA and t-SNE Plots](fig/tsne.png)

Intersectional Bias on `Adult-Income` dataset.

![Intersectional Bias](fig/ib.png)

### Model's performance on Data utiliy vs Fairness

Here, we have compared all the model in our benchmarking tools on Data utility vs Fairness metrics. For the data utlity, we calculate the Accuracy and for the fairness, we measure Demographic Parity Ration (DPR) and Equilized Odds Ratio (EOR), and plot them in 3d.

![Fairness vs Data utlity](fig/fairnessvsdata.png)

More results coming soon!

## References

1. https://github.com/fairlearn/fairlearn
2. https://github.com/Trusted-AI/AIF360
3. https://github.com/shap/shap